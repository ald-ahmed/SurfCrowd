{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SurfCrowd - Realtime Surfer Detection Model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ald-ahmed/SurfCrowd/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6zJeetst2T8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# SurfCrowd - A Realtime Surfer Detection Model\n",
        "\n",
        "\n",
        "The following work showcases a yolo-based model that has been trained on surfing live cams. The output shows bounding boxes around surfers, enabling tracking and/or counting abilities in realtime. Powered by [Surfline](https://www.surfline.com) and created by [A. Dulaimy](https://twitter.com/dulaimyy)\n",
        "\n",
        "\n",
        " <br /> \n",
        "*To run the  entire notebook, go to Runtime -> Run all*\n",
        " <br /> \n",
        " <br /> \n",
        "\n",
        "---\n",
        "<br/> \n",
        "\n",
        "Example output on Upper Trestles live cam ([source](https://www.surfline.com/surf-report/upper-trestles/5842041f4e65fad6a7708887))\n",
        "\n",
        " <br /> \n",
        "\n",
        "<div>\n",
        "<img src=\"https://storage.googleapis.com/ahmed.software/projects/surfcrowd2-min.gif\" width=\"640\"/>\n",
        "</div>\n",
        "\n",
        "<br/> \n",
        "<br/> \n",
        "\n",
        "Performance metrics on a small dataset after 300 epochs of training (~1000 images with 15-20 surfers)\n",
        "\n",
        "<br/> \n",
        "\n",
        "<div>\n",
        "<img src=\"http://ahmed.software/projects/crowd%20surf%20performance.png\" width=\"640\"/>\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb",
        "colab_type": "text"
      },
      "source": [
        "#Install Dependencies\n",
        "\n",
        "*Choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1af18c3-a3ed-41f2-9d72-4fe4ab3a447e"
      },
      "source": [
        "!git clone https://github.com/ald-ahmed/SurfCrowd  # clone repo\n",
        "!pip install -r SurfCrowd/requirements.txt\n",
        "\n",
        "%cd SurfCrowd\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output \n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.5.1+cu101 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15079MB, multi_processor_count=40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJx-2NHsYxT",
        "colab_type": "text"
      },
      "source": [
        "# Model Configuration and Architecture\n",
        "\n",
        "Here is a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n",
        "\n",
        "*This is here for informational purposes only. You do not need to edit these cells.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDxebz13RdRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "efe9d091-bca8-4747-fa7e-6c6c57822a26"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "num_classes = \"1\"\n",
        "\n",
        "##write custom model .yaml\n",
        "#you can configure this based on other YOLOv5 models in the models directory\n",
        "with open('SurfCrowd/models/custom_yolov5s.yaml', 'w') as f:\n",
        "  # parameters\n",
        "  f.write('nc: ' + num_classes + '\\n')\n",
        "  #f.write('nc: ' + str(len(class_labels)) + '\\n')\n",
        "  f.write('depth_multiple: 0.33'  + '\\n') # model depth multiple\n",
        "  f.write('width_multiple: 0.50'  + '\\n')  # layer channel multiple\n",
        "  f.write('\\n')\n",
        "  f.write('anchors:' + '\\n')\n",
        "  f.write('  - [10,13, 16,30, 33,23] ' + '\\n')\n",
        "  f.write('  - [30,61, 62,45, 59,119]' + '\\n')\n",
        "  f.write('  - [116,90, 156,198, 373,326] ' + '\\n')\n",
        "  f.write('\\n')\n",
        "\n",
        "  f.write('backbone:' + '\\n')\n",
        "  f.write('  [[-1, 1, Focus, [64, 3]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [128, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 3, Bottleneck, [128]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [256, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 9, BottleneckCSP, [256]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [512, 3, 2]], ' + '\\n')\n",
        "  f.write('   [-1, 9, BottleneckCSP, [512]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [1024, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 1, SPP, [1024, [5, 9, 13]]],' + '\\n')\n",
        "  f.write('   [-1, 6, BottleneckCSP, [1024]],' + '\\n')\n",
        "  f.write('  ]' + '\\n')\n",
        "  f.write('\\n')\n",
        "\n",
        "  f.write('head:'  + '\\n')\n",
        "  f.write('  [[-1, 3, BottleneckCSP, [1024, False]],'  + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  f.write('   [-2, 1, nn.Upsample, [None, 2, \"nearest\"]],' + '\\n')\n",
        "  \n",
        "  f.write('   [[-1, 6], 1, Concat, [1]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [512, 1, 1]],' + '\\n')\n",
        "  f.write('   [-1, 3, BottleneckCSP, [512, False]],' + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  \n",
        "  f.write('   [-2, 1, nn.Upsample, [None, 2, \"nearest\"]],' + '\\n')\n",
        "  f.write('   [[-1, 4], 1, Concat, [1]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [256, 1, 1]],' + '\\n')\n",
        "  f.write('   [-1, 3, BottleneckCSP, [256, False]],' + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  f.write('\\n' )\n",
        "  f.write('   [[], 1, Detect, [nc, anchors]],' + '\\n')\n",
        "  f.write('  ]' + '\\n')\n",
        "\n",
        "print('custom model config written!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "custom model config written!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfr3BZaVwCKp",
        "colab_type": "text"
      },
      "source": [
        "# Predict\n",
        "\n",
        "The following will load the pretrained weigths into the model and  generate an output for each frame in the input video. Each frame takes around 0.020s to process, which translates to 50 fps!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9CH20dnwA2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/SurfCrowd/\n",
        "!rm -rf /inference/output\n",
        "\n",
        "# predict using the pretrained weights. Note the image size, confidence, and source parameters  \n",
        "!python detect.py --weights weights/last_yolov5s_results.pt --img 1920 --save-txt --conf 0.4 --source data/test/video/1.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_xZCR1CxKWp",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "import glob\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inferedData = []\n",
        "for txtName in glob.glob('/content/SurfCrowd/inference/output/*.txt'):\n",
        "    num_lines = sum(1 for line in open(txtName))  \n",
        "    inferedData.append(num_lines)\n",
        "\n",
        "stats = pd.Series(inferedData).describe()\n",
        "print(stats)\n",
        "plt.hist(inferedData, bins=30, alpha=1, histtype='stepfilled', color='steelblue');\n",
        "\n",
        "# # uncomment this if images were inputed \n",
        "# from IPython.display import Image, display\n",
        "# for imageName in glob.glob('/content/SurfCrowd/inference/output/*.jpg'):\n",
        "#     display(Image(filename=imageName))\n",
        "#     print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtrLVgM7xe7C",
        "colab_type": "text"
      },
      "source": [
        "Run this cell to show the generated video if one was used in the input. This will take a bit given the need to convert/compress the generated video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCRvAXGqpBsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "# compress the outputed video to display it\n",
        "save_path = \"/content/SurfCrowd/inference/output/1.mp4\"\n",
        "compressed_path = \"/content/compressed1.mp4\"\n",
        "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "# Show the outputed video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=640 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}